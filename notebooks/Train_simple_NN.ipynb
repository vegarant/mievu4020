{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data and create dataset objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/itf-fi-ml/home/vegarant/.local/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:174.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Dataset\n",
    "We can index Datasets manually like a list: `training_data[index]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape:  torch.Size([1, 28, 28])\n",
      "img.dtype:  torch.float32\n",
      "img.device:  cpu\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "\n",
    "sample_idx = 102\n",
    "\n",
    "img, label = training_data[sample_idx]\n",
    "print('img.shape: ',  img.shape)\n",
    "print('img.dtype: ',  img.dtype)\n",
    "print('img.device: ', img.device) # Notice that the data is lying on the CPU. This is standard.\n",
    "\n",
    "plt.title(labels_map[label], fontsize=20)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataloaders\n",
    "The `Dataset` retrieves our dataset’s features and labels one sample at a time. When training a model, we typically want to pass samples in “minibatches” and reshuffle the data at every epoch to reduce model overfitting and use Python’s `multiprocessing` to speed up data retrieval.\n",
    "\n",
    "`DataLoader` is an iterable that abstracts this complexity for us in an easy API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "shuffle=True\n",
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=shuffle)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Iterate through the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 3\n"
     ]
    }
   ],
   "source": [
    "# Display image and label.\n",
    "train_input, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Input batch shape: {train_input.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_input[0].squeeze()\n",
    "label = int(train_labels[0])\n",
    "\n",
    "plt.title(labels_map[label], fontsize=20)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model\n",
    "We define our neural network by subclassing `nn.Module`, and initialize the neural network layers in `__init__`. Every `nn.Module` subclass implements the operations on input data in the forward method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Create all the network layers (make sure the dimensions add up)\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(in_features=28*28, out_features=512, bias=True)\n",
    "        self.r1  = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=512, bias=True)\n",
    "        self.r2  = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(in_features=512, out_features=10, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Specify how the layers should be connected\n",
    "        x = self.flatten(x) # Reshape the input from 28x28 matrices to vectors with length 28*28=784\n",
    "        x = self.fc1(x)     # Input must be flattend when using fully connecte layers\n",
    "        x = self.r1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.r2(x)\n",
    "        logits = self.fc3(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create network instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (r1): ReLU()\n",
      "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (r2): ReLU()\n",
      "  (fc3): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
       "  (r1): ReLU()\n",
       "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (r2): ReLU()\n",
       "  (fc3): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a few hyperparameters\n",
    "device = torch.device('cuda')\n",
    "epochs = 5 # Number of iterations with stochastic gradient descent.\n",
    "learning_rate = 0.003\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() # Define the loss funciton\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # Choose the optimizer \n",
    "model.to(device) # Move model parameters to the device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the train loop\n",
    "\n",
    "Inside the training loop, optimization happens in three steps:\n",
    " * Call `optimizer.zero_grad()` to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\n",
    " * Backpropagate the prediction loss with a call to `loss.backward()`. PyTorch deposits the gradients of the loss w.r.t. each parameter.\n",
    " * Once we have our gradients, we call `optimizer.step()` to adjust the parameters by the gradients collected in the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    batch = 0\n",
    "    for x, y in dataloader:\n",
    "        # Compute prediction and loss\n",
    "        batch = batch + 1\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() # A common mistake is to forget to call this function.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(x)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def validation_loop(dataloader, model, loss_fn, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    val_loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            \n",
    "            val_loss += loss_fn(pred, y).item() # This is a float\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    val_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Validataion Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {val_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.261883  [ 6400/60000]\n",
      "loss: 2.222661  [12800/60000]\n",
      "loss: 2.151118  [19200/60000]\n",
      "loss: 2.095855  [25600/60000]\n",
      "loss: 1.965482  [32000/60000]\n",
      "loss: 1.864837  [38400/60000]\n",
      "loss: 1.796638  [44800/60000]\n",
      "loss: 1.605005  [51200/60000]\n",
      "loss: 1.476355  [57600/60000]\n",
      "Validataion Error: \n",
      " Accuracy: 61.5%, Avg loss: 1.480712 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.328611  [ 6400/60000]\n",
      "loss: 1.283706  [12800/60000]\n",
      "loss: 1.218757  [19200/60000]\n",
      "loss: 1.139306  [25600/60000]\n",
      "loss: 1.121738  [32000/60000]\n",
      "loss: 1.084680  [38400/60000]\n",
      "loss: 0.950377  [44800/60000]\n",
      "loss: 0.902946  [51200/60000]\n",
      "loss: 0.935013  [57600/60000]\n",
      "Validataion Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.972756 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.718536  [ 6400/60000]\n",
      "loss: 0.917980  [12800/60000]\n",
      "loss: 0.846074  [19200/60000]\n",
      "loss: 0.750468  [25600/60000]\n",
      "loss: 0.864024  [32000/60000]\n",
      "loss: 0.856481  [38400/60000]\n",
      "loss: 0.864432  [44800/60000]\n",
      "loss: 0.806691  [51200/60000]\n",
      "loss: 0.866853  [57600/60000]\n",
      "Validataion Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.817444 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.789048  [ 6400/60000]\n",
      "loss: 0.717461  [12800/60000]\n",
      "loss: 0.676836  [19200/60000]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'Image'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4034cc5ca582>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mvalidation_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-f04d1d1dee0b>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer, device)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;31m# Compute prediction and loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/storage/software/PyTorch/1.9.0-fosscuda-2020b/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/storage/software/PyTorch/1.9.0-fosscuda-2020b/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/storage/software/PyTorch/1.9.0-fosscuda-2020b/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/storage/software/PyTorch/1.9.0-fosscuda-2020b/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \"\"\"\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mmode_to_nptype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'I'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'I;16'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'F'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     img = torch.from_numpy(\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_to_nptype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     )\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'Image'"
     ]
    }
   ],
   "source": [
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer, device)\n",
    "    validation_loop(test_dataloader, model, loss_fn, device)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
