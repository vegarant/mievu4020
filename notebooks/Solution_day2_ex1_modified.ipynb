{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data and create dataset objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/itf-fi-ml/home/vegarant/.local/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:174.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Dataset\n",
    "We can index Datasets manually like a list: `training_data[index]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape:  torch.Size([1, 28, 28])\n",
      "img.dtype:  torch.float32\n",
      "img.device:  cpu\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "\n",
    "sample_idx = 105\n",
    "\n",
    "img, label = training_data[sample_idx]\n",
    "print('img.shape: ',  img.shape)\n",
    "print('img.dtype: ',  img.dtype)\n",
    "print('img.device: ', img.device) # Notice that the data is lying on the CPU. This is standard.\n",
    "\n",
    "plt.title(labels_map[label], fontsize=20)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataloaders\n",
    "The `Dataset` retrieves our dataset’s features and labels one sample at a time. When training a model, we typically want to pass samples in “minibatches” and reshuffle the data at every epoch to reduce model overfitting and use Python’s `multiprocessing` to speed up data retrieval.\n",
    "\n",
    "`DataLoader` is an iterable that abstracts this complexity for us in an easy API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "shuffle=True\n",
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=shuffle)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Iterate through the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "# Display image and label.\n",
    "train_input, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Input batch shape: {train_input.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_input[0].squeeze()\n",
    "label = int(train_labels[0])\n",
    "\n",
    "plt.title(labels_map[label], fontsize=20)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model\n",
    "We define our neural network by subclassing `nn.Module`, and initialize the neural network layers in `__init__`. Every `nn.Module` subclass implements the operations on input data in the forward method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Create all the network layers (make sure the dimensions add up)\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding='same')\n",
    "        self.r1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=30, kernel_size=3, padding=\"same\")\n",
    "        self.r2 = nn.ReLU()\n",
    "        self.conv3 = nn.Conv2d(in_channels=30, out_channels=6, kernel_size=3, padding=\"same\")\n",
    "        self.r3 = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(in_features=28*28*6, out_features=10)\n",
    "        \n",
    "        #self.sig1 = nn.Sigmoid()\n",
    "        #self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        #self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, padding='valid')\n",
    "        #self.sig2 = nn.Sigmoid()\n",
    "        #self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        #self.flatten = nn.Flatten()\n",
    "        #self.fc1 = nn.Linear(in_features=5*5*16, out_features=120, bias=True)\n",
    "        #self.sig3  = nn.Sigmoid()\n",
    "        #self.fc2 = nn.Linear(in_features=120, out_features=84, bias=True)\n",
    "        #self.sig4  = nn.Sigmoid()\n",
    "        #self.fc3 = nn.Linear(in_features=84, out_features=10, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Specify how the layers should be connected\n",
    "        x = self.conv1(x) # [1,28,28] -> [6,28,28]\n",
    "        x1 = self.r1(x)    # [6,28,28] -> [6,28,28]\n",
    "        x = self.conv2(x1) # [6,28,28] -> [30,28,28]\n",
    "        x = self.r2(x)    # [30,28,28] -> [30,28,28]\n",
    "        x = self.conv3(x) # [30,28,28] -> [6,28,28]\n",
    "        x = self.r3(x) + x1\n",
    "        x = self.flatten(x) # [6,28,28] -> [6*28*28]\n",
    "        x = self.fc1(x) # [6*28*28] -> [10]\n",
    "        return x\n",
    "        #x = self.sig1(x)\n",
    "        #x = self.pool1(x)\n",
    "        #x = self.conv2(x)\n",
    "        #x = self.sig2(x)\n",
    "        #x = self.pool2(x)\n",
    "        #x = self.flatten(x)\n",
    "        #x = self.fc1(x)     \n",
    "        #x = self.sig3(x)\n",
    "        #x = self.fc2(x)\n",
    "        #x = self.sig4(x)\n",
    "        #logits = self.fc3(x)\n",
    "        #return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create network instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()\n",
    "#print(model)\n",
    "\n",
    "x = torch.zeros([1,1,28,28])\n",
    "y = model(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
       "  (r1): ReLU()\n",
       "  (conv2): Conv2d(6, 30, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "  (r2): ReLU()\n",
       "  (conv3): Conv2d(30, 6, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "  (r3): ReLU()\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=4704, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a few hyperparameters\n",
    "device = torch.device('cuda')\n",
    "epochs = 5 # Number of iterations with stochastic gradient descent.\n",
    "learning_rate = 0.003\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() # Define the loss funciton\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # Choose the optimizer \n",
    "model.to(device) # Move model parameters to the device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the train loop\n",
    "\n",
    "Inside the training loop, optimization happens in three steps:\n",
    " * Call `optimizer.zero_grad()` to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\n",
    " * Backpropagate the prediction loss with a call to `loss.backward()`. PyTorch deposits the gradients of the loss w.r.t. each parameter.\n",
    " * Once we have our gradients, we call `optimizer.step()` to adjust the parameters by the gradients collected in the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    batch = 0\n",
    "    for x, y in dataloader:\n",
    "        # Compute prediction and loss\n",
    "        batch = batch + 1\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() # A common mistake is to forget to call this function.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(x)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def validation_loop(dataloader, model, loss_fn, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    val_loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            \n",
    "            val_loss += loss_fn(pred, y).item() # This is a float\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    val_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Validataion Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {val_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.372285  [ 6400/60000]\n",
      "loss: 1.015524  [12800/60000]\n",
      "loss: 0.688640  [19200/60000]\n",
      "loss: 0.815536  [25600/60000]\n",
      "loss: 0.651295  [32000/60000]\n",
      "loss: 0.708011  [38400/60000]\n",
      "loss: 0.765028  [44800/60000]\n",
      "loss: 0.446939  [51200/60000]\n",
      "loss: 0.492830  [57600/60000]\n",
      "Validataion Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.679210 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.629153  [ 6400/60000]\n",
      "loss: 1.053083  [12800/60000]\n",
      "loss: 0.522496  [19200/60000]\n",
      "loss: 0.549569  [25600/60000]\n",
      "loss: 0.616990  [32000/60000]\n",
      "loss: 0.469258  [38400/60000]\n",
      "loss: 0.305472  [44800/60000]\n",
      "loss: 0.522825  [51200/60000]\n",
      "loss: 0.482873  [57600/60000]\n",
      "Validataion Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.602204 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.360897  [ 6400/60000]\n",
      "loss: 0.442546  [12800/60000]\n",
      "loss: 0.437860  [19200/60000]\n",
      "loss: 0.318695  [25600/60000]\n",
      "loss: 0.489805  [32000/60000]\n",
      "loss: 0.589756  [38400/60000]\n",
      "loss: 0.709243  [44800/60000]\n",
      "loss: 0.663712  [51200/60000]\n",
      "loss: 0.535144  [57600/60000]\n",
      "Validataion Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.520741 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.632334  [ 6400/60000]\n",
      "loss: 0.807259  [12800/60000]\n",
      "loss: 0.350239  [19200/60000]\n",
      "loss: 0.469709  [25600/60000]\n",
      "loss: 0.455782  [32000/60000]\n",
      "loss: 0.436815  [38400/60000]\n",
      "loss: 0.367708  [44800/60000]\n",
      "loss: 0.548497  [51200/60000]\n",
      "loss: 0.291871  [57600/60000]\n",
      "Validataion Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.505448 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.368751  [ 6400/60000]\n",
      "loss: 0.640238  [12800/60000]\n",
      "loss: 0.346552  [19200/60000]\n",
      "loss: 0.272578  [25600/60000]\n",
      "loss: 0.365267  [32000/60000]\n",
      "loss: 0.304788  [38400/60000]\n",
      "loss: 0.567041  [44800/60000]\n",
      "loss: 0.426571  [51200/60000]\n",
      "loss: 0.463857  [57600/60000]\n",
      "Validataion Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.482200 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer, device)\n",
    "    validation_loop(test_dataloader, model, loss_fn, device)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
